\section{Reflection Perspective}
\subsection{Evolution and Refactoring}
\subsubsection{Introduction of technical debt}
One of the challenges we faced during the refactoring phase of the MiniTwit project was the accumulation of technical debt. Refactoring often took longer than anticipated, and tasks that werenâ€™t tracked tended to be forgotten. To address this, we created some informal meetings after our lecture, where we could review progress, create new tasks on GitHub issues, and assign the responsible person for the tasks. Although this helped, in hindsight, we should have enforced the creation of issues a bit more, especially during the later stages of the project.

\subsection{Operation}\label{operation}
\subsubsection{Adding indexes}
As we monitored the performance of our API and web application throughout the project, we noticed a degradation in response times. At one point, the \texttt{/public} endpoint was particularly affected, with average response times reaching up to 70 seconds. This was unacceptable, and we began investigating the cause.
\\\\
Although we had not implemented database query monitoring through prometheus, we suspected that the bottleneck might lie in our SQL queries. Looking at the performance metrics provided by DigitalOcean, where our PostgreSQL database is hosted, we identified that the \texttt{queryMessages} method was the cause of this issue, and it became clear that we needed to index on frequently queried columns. The indexation resolved the issue, and our \texttt{/public} endpoint has since then had a response time of 20-50ms. In future projects, it will be optimal to monitor the database execution time and visualize it as a \texttt{Time series} in Grafana to see if queries degrade over time.

\subsubsection{API terminating}
The very first issue we encountered was the API becoming unavailable due to crashing when the database encountered an error. We had a hard time debugging this, as the crash did not provide any valuable information in the Docker logs (a logging tool had not been introduced yet), only to discover that we used \texttt{panic}, which causes the program to terminate in Go. This took a bit of time to realize, but when we finally traced the issue, we replaced the panic call with returning a proper status code and tried to enforce more thorough code reviews, better logging and although implemented a bit late, we integrated our testing suite into our CI flow as shown in section \ref{cicd}.

\subsection{Maintenance}\label{maintainence}
\subsubsection{Disk usage}
One valuable lesson we have learned during the course was the importance of managing disk space in production. At one point, we lost the ability to SSH into our main droplet. We used the recovery console to investigate the issue and discovered that the disk was completely full. After analyzing the disk usage, we found that whenever we deployed a new version of minitwit, we pulled the latest image, but didn't delete the old one, which slowly consumed all the storage. To resolve this, we removed the old images, scaled the storage vertically, and have since been more observant of disk usage. Although not yet implemented, it would also be a good idea to prune old unused images automatically when pulling new ones.