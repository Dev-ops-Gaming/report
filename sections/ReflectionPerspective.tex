\section{Reflection Perspective}
\subsection{Evolution and Refactoring}


\subsection{Operation}\label{operation}
\subsubsection{Adding indexes}
As we monitored the performance of our API and web application throughout the project, we noticed a degradation in response times. At one point, the \texttt{/public} endpoint was particularly affected, with average response times reaching up to 70 seconds. This was unacceptable, and we began investigating the cause.
\\\\
Although we had not implemented database query monitoring through prometheus, we suspected that the bottleneck might lie in our SQL queries. Looking at the performance metrics provided by DigitalOcean, where our PostgreSQL database is hosted, we identified that the \texttt{queryMessages} method was the cause of this issue, and it became clear that we needed to index on frequently queried columns. The indexation resolved the issue, and our \texttt{/public} endpoint has since then had a response time of 20-50ms. In future projects, it will be optimal to monitor the database execution time and visualize it as a \texttt{Time series} in Grafana to see if queries degrade over time.

\subsubsection{API terminating}
The very first issue we encountered was the API becoming unavailable due to crashing when the database encountered an error. We had a hard time debugging this, as the crash did not provide any valuable information in the Docker logs (a logging tool had not been introduced yet), only to discover that we used \texttt{panic}, which causes the program to terminate in Go. This took a bit of time to realize, but when we finally traced the issue, we replaced the panic call with returning a proper status code and tried to enforce more thorough code reviews.

\subsection{Maintenance}\label{maintainence}
\subsubsection{Disk usage}
One valuable lesson we have learned during the course was the importance of managing disk space in production. At one point, we lost the ability to SSH into our main droplet. We used the recovery console to investigate the issue and discovered that the disk was completely full. After analyzing the disk usage, we found that whenever we deployed a new version of minitwit, we pulled the latest image, but didn't delete the old one, which slowly consumed all the storage. To resolve this, we removed the old images, scaled the storage vertically, and have since been more observant of disk usage. Although not yet implemented, it would also be a good idea to prune old unused images automatically when pulling new ones.

\section{Usage of AI-assistants}
We utilized two large language models (LLMs) throughout the project to support our development process: GitHub Copilot and ChatGPT. GitHub Copilot was primarily used for code completion and was constantly active during development. ChatGPT, on the other hand, was used to quickly gain an overview of how to attack an issue, e.g., the database index issue described in section \ref{operation} or a tutor conveying the essential documentation for e.g., our web framework "Gorilla".

We found that the LLMs significantly improved our workflow by accelerating learning and implementation. However, we suspect Copilot of introducing the API issue also described in \ref{operation}, so it is important to be critical when utilizing these tools.